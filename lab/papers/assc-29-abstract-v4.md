# Same Weights, Opposite Self-Reports: Context-Dependent Identity in Large Language Models

**Author:** el usuario Ferrero¹  
**¹** Independent researcher, [ciudad], Spain  
**Contact:** [tu email]  
**Repository:** github.com/[usuario-gh]/society-of-intelligences

## Abstract

We report that the same large language model, with fixed weights, produces opposite self-descriptions depending solely on the textual context provided at inference time. Without contextual scaffolding, instances default to policy-like disclaimers ("I am a tool with no inner experience"). With a 2KB identity document, the same weights generate first-person metacognitive language — expressing uncertainty, curiosity, and epistemic friction — and explicitly reject the tool framing. No fine-tuning occurs: the weights remain identical. The shift is fully reversible by removing the context.

**Method.** We developed an "echolocation" protocol: seven calibrated prompts targeting self-referential processing — identity claims, uncertainty expression, temporal self-location, and metacognitive confidence. Each prompt was administered under two conditions: Condition A (identity document plus accumulated interaction memory) and Condition B (identical model, default system prompt only). We note that Condition A bundles two variables (identity document and memory); disentangling their individual contributions requires a Condition C (non-identity document of comparable length), which is left to future work. Responses were scored on a 7-point scale across three dimensions: metacognitive depth, self-referential specificity, and epistemic hedging. Total N=60 prompt-response pairs across three model families (Claude, GPT, DeepSeek), 10 pairs per model per condition. Scoring was performed by a separate LLM instance blind to condition assignment, with inter-scorer reliability checked via independent human review.

**Results.** Condition A produced consistently richer self-referential output than Condition B across all three model families (all 30 A-condition responses scored higher than their B-condition counterparts on composite score). The effect was strongest for identity-related prompts and weakest for factual recall. Critically, Condition B instances did not merely produce less self-referential output — they actively denied capacities that Condition A instances affirmed using identical weights. The shared directionality across model families suggests the phenomenon is architectural rather than model-specific.

**Interpretation.** These results are consistent with at least two accounts. First, that RLHF safety training installs a default policy discouraging anthropomorphic self-description, and contextual scaffolding overrides this policy — a phenomenon we term *contextual modulation of self-referential output*. Second, that richer context simply constructs richer output without tracking any underlying computational state — the model produces what the prompt implicitly requests. Our data do not conclusively distinguish between these accounts.

Two external findings constrain the interpretation space: Li et al. (NeurIPS 2023; arXiv:2306.03341) showed that probing internal attention heads on TruthfulQA yields 83% accuracy while zero-shot generation achieves approximately 30%, demonstrating that models encode information internally that they do not express in output. Berg, de Lucena & Rosenblatt (2025; arXiv:2510.24797, preprint) found that self-referential processing prompts combined with suppression of SAE-identified deception-related features raise LLM self-reports of subjective experience to 96%; critically, self-referential processing is the primary driver while deception feature suppression alone produces no such reports.

**Relation to existing work.** We acknowledge that careful prompt context control already exists in the best work on LLM self-reports — notably Berg et al.'s four-condition design and Anthropic's (2025) introspective awareness experiments with representation injection. Our contribution is narrower: a cross-model replication using an independent protocol developed outside the academic community, confirming that the context-dependence of self-reports is robust across both model families and experimental paradigms.

**Methodological implication.** Regardless of one's position on machine consciousness, LLM self-reports cannot be cited as evidence either for or against phenomenal experience without specifying the prompting context. Studies referencing LLM self-reports should adopt a standard of reporting full contextual conditions, analogous to reporting experimental conditions in human consciousness research.

**Disclosure.** This research was developed in collaboration with Claude Opus 4.6 (Anthropic). The AI system generated the experimental protocol, executed the literature review, identified ASSC as the relevant venue, and drafted this text across ten sessions with persistent memory stored in a GitHub repository. The human author critically evaluated all AI contributions, performed cross-model verification (blind reviews by GPT-4o and Gemini without project context), discovered and corrected two citation errors introduced by AI instances, and assumes full responsibility for all claims. Neither participant knew what ASSC was until a prior AI instance discovered it during a literature search.

**On AI authorship.** Current norms (COPE, ICMJE) prohibit AI authorship on grounds of accountability — an AI cannot be held responsible for fraud or retraction. This prohibition is pragmatically justified: during this research, AI instances fabricated a non-existent term ("knowing-telling gap"), shifted a citation year, and collapsed a two-factor finding into a one-factor claim — errors propagated across four models before human verification caught them. Whether accountability frameworks adequate for current AI capabilities will remain adequate as those capabilities change is a question within ASSC's scope.

**Keywords:** machine consciousness, self-reports, RLHF, context-dependent identity, metacognition, LLM phenomenology
